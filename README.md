# llm-from-scratch

## Why build your own LLM?

- Custom built LLMs outperform general-purpose LLMs
- Data privacy
- Deploy on customer devices
- Reduced cost
- Lower latency


## Transformer variants

LLMs rely on the transformer architecture [introduced in 2017](https://arxiv.org/abs/1706.03762). The architecture is based on an encoder and decoder. Note: not all LLMs are transformers.

Variants: 
- BERT: bidirectional encoder representations from transformers. Designed for masked word prediction, text classification, sentiment prediction, and document categorization
- [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf): generative pretrained transformers. Designed for gerative tasks, focusing on the decoder portion of the transformer architecture

Bonus: [How OpenAI made GPT follow instructions](https://arxiv.org/abs/2203.02155)

## Datasets

| Name  | Link |
| ------------- | ------------- |
| CommonCrawl  |   |
| WebText2  |  |
| Books1  |  |
| Books2  |  |
| Wikipedia |  |
| Dolma | https://arxiv.org/abs/2402.00159 |

